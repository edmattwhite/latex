\documentclass{tufte-handout}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage[shortlabels]{enumitem}

\title{$p$-Values}
\author{}


% LISTINGS STYLE
\lstdefinestyle{code}{
    belowcaptionskip=1\baselineskip,
    breaklines=true,
    frame=none,
    numbers=none,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\bfseries\color{green!40!black},
    commentstyle=\itshape\color{purple!40!black},
    identifierstyle=\color{blue},
    backgroundcolor=\color{gray!10!white},
}
\lstset{style=code}

% CITATION STYLING (VANCOUVER)
\usepackage{natbib}
\setcitestyle{numbers}

% THEOREM STYLING
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{qn}{Question}
\theoremstyle{definition}
\newtheorem{dfn}{Definition}
\newtheorem{exm}{Example}

% COMMON COMMANDS
\newcommand{\rlang}{\textbf{R}}
\newcommand{\XX}{\mathbf{X}}
\newcommand{\YY}{\mathbf{Y}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\idt}{\perp\!\!\!\perp}
\newcommand{\nidt}{\not\!\perp\!\!\!\perp}
\newcommand{\prb}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\dty}[1]{p \left( #1 \right)}
\newcommand{\exn}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\lik}[1]{\mathcal{L}\left( #1 \right)}
\newcommand{\loglik}[1]{\mathfrak{l}\left( #1 \right)}
\newcommand{\mle}[1]{\tilde{#1}}
\newcommand{\unb}[1]{\hat{#1}}
\newcommand{\dnormal}[2]{\mathcal{N}\left( #1, #2 \right)}
\newcommand{\dpoisson}[1]{\text{Poisson}\left( #1 \right)}
\newcommand{\dchisq}[1]{\chi^2_{#1}}
\newcommand{\dt}[1]{t_{#1}}
\newcommand{\dF}[2]{F_{#1, #2}}

% FORMATTING
\setlength{\parskip}{0.75em}
\renewcommand\vec{\mathbf}
\floatsep 8pt
\usepackage[font=footnotesize,labelfont=bf]{caption}

\begin{document}
\maketitle

\begin{abstract}
    \noindent
    $p$-values need to be defined carefully to avoid miscommunication, but if you do understand them they're a neat summarisation.
\end{abstract}

\section{Why do we talk about $p$-values?}

The scientific method is built around the idea of formulating and testing hypotheses.
Statistics can give us a way of \textit{testing} or \textit{comparing} hypotheses \cite{casella2021statistical}.

Historically, frequentist statistics has been the framework that these tests have been constructed under.
This involves the assumption that our data are random, and that they are generated by some underlying process which has fixed parameters.
Test statistics (such as $z$-scores and $t$-scores) are calculated from our data and are useful excatly because we know how they are distributed under our hypothesis.

What is distinctly less useful is that different test statistics are very hard to compare -- if we see $t$-scores published then how can we compare them with a different test statistic, like that from a Kolmogorov-Smirnov test?

The $p$-value attempts to solve this problem and make different tests comparable in a meaningful way.
Unfortunately, because of the layers of abstraction that are built upon and the way that people like to think about probability statements, it's often misunderstood.

\section{How do we get there?}

Let's step back for a second and consider what we're doing.
In our experiment we're taking a sample $\XX$ from some population, and we're trying to learn something about the parameters $\theta$ that govern the population from that sample.

We have decided that the best way to do this is by formulating an idea and testing to see whether it's true or not.\footnote{It's worth noting that we could have done something else -- for example run a parameter estimation procedure -- hypothesis testing is not the beginning and end of statistical inference!}

\begin{dfn}[Hypothesis]
    A hypothesis $H$ is a statement about our belief in possible values of the population parameter: $\theta \in \Theta$.
\end{dfn}

\begin{dfn}[Hypothesis Test]
    A hypothesis test comparing two hypotheses $H_0$ and $H_1$ is a procedure that specifies:
    \begin{enumerate}
        \item For which sample values the decision is made to accept $H_0$.
        \item For which sample values $H_0$ is rejected and $H_1$ is accepted as true.
    \end{enumerate}
\end{dfn}

Given that our $X$ are randomly generated, even if we had perfect knowledge of $\theta$ then we won't know the exact value for $\mathbf{X}$.
This also carries across to any test statistic that we might construct from $X$.
To try and make this easier to deal with, a $p$-value combines the idea of a test statistic with that test statistic's observation probability under $H_0$:

\begin{dfn}[$p$-value]
    A $p$-value $p(\XX)$ is a test statistic satisfying $p(\XX) \in [0, 1]$ for every sample point $\XX$.
    A valid $p$-value has that for every $\theta \in \Theta_0$, and every $\gamma \in [0, 1]$:
    $$ \prb{p(\XX) \leq \gamma \vert \theta} \leq \gamma $$
\end{dfn}

Importantly, note that our definition of the $p$-value doesn't make any kind of probabilistic statement about the veracity of $H_0$.

\textbf{A $p$-value simply states the probability of observing data at least as `unusual' as the sample given $H_0$.}

\section{Why are they useful?}

If we're performing a test, then regardless of the underlying distributional assumptions of that test, we should be able to find a $p$-value to communicate the result of testing a particular sample.
This gives us a way to more easily compare different testing procedures \footnote{However we do still need to be aware of the procedures and assumptions behind different tests.}, which was our primary goal.
One more thing that the $p$-value gives us is the ability to set our own personal threshold for what we believe an extreme value for a test statistic should be.
Results in particle physics are often considered significant only once they've reached the ``5-sigma'' level (which is a $p$-value $< 3.5 \times 10 ^ {-7}$), whilst social scientists often talk about $p$-values $< 0.05$ as being of interest.
The $p$-value allows us to draw a line under which we're happy to start talking about an effect as being statistically significant, and ensures that line is able to be compared across different experiments.

\section{What's the problem?}

Apart from the fact that people are really bad at understanding what $p$-values actually mean, there's a bigger problem here.
Let's say that I start accepting any scientific study that has observed an effect (or rejected $H_0$) with a $p$-value less than some threshold (say 0.05).
Then, by the definition of the $p$-value, if I were to take a collection of 20 findings that I believed were true, I would expect that in fact one of them was false.

By itself this is not awful -- if 95\% of published findings were correct then that would be great!
However, the problem comes when publishing is made conditional on a $p$-value threshold, as in that case a large enough community interested in a phenomenon will be certain to generate an experimental result with the requisite $p$-value after some time.
This can't be put in context because anyone performing an experiment and not finding the evidence to support a new hypothesis won't have been incentivised to make that `non-discovery' widely known.
In this world, none of our scientists have done anything wrong, but an over reliance on the $p$-value as a threshold has led to an undesirable result, and people misunderstanding $p$-values (as some probability of $H_0$ being true) means that what is published is given the wrong interpretation!

\bibliographystyle{unsrtnat}
\bibliography{main}

\end{document}